{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63428dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from Bio import SeqIO\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4869ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences from the FASTA file\n",
    "fasta_file = \"./data/uniprot_sprot.fasta\"\n",
    "sequences = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "print(\"Example:\", sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the amino acid vocabulary and special tokens for BERT\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWYUOBZX\" # standard 20 amino acids + 5 non-standard amino acids (U, O, B, Z, X)\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]  # special tokens for BERT\n",
    "vocab_dict = {token: idx for idx, token in enumerate(list(amino_acids) + special_tokens)}\n",
    "\n",
    "# Initialize the BERT tokenizer with the custom vocabulary\n",
    "tokeniser = BertTokenizerFast(\n",
    "    vocab=vocab_dict,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# Tokenise the sequences\n",
    "encodings = tokeniser(sequences, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "print(encodings[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset class for the tokenised sequences\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings[\"input_ids\"].shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "# Create the dataset from encodings\n",
    "dataset = ProteinDataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT configuration and model initialisation\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokeniser),\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=tokeniser.pad_token_id,\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418083ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokeniser, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_bert\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialise the Trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokeniser\n",
    "model.save_pretrained(\"./mlm_bert\")\n",
    "tokeniser.save_pretrained(\"./mlm_bert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
