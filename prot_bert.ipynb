{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bcaebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 492/492 [00:00<00:00, 890.45it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "The tied weights mapping and config for this model specifies to tie bert.embeddings.word_embeddings.weight to cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie cls.predictions.bias to cls.predictions.decoder.bias, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "\u001b[1mBertForMaskedLM LOAD REPORT\u001b[0m from: Rostlab/prot_bert\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11088494211435318,\n",
       "  'token': 5,\n",
       "  'token_str': 'L',\n",
       "  'sequence': 'D L I P T S S K L V V L D T S L Q V K K A F F A L V T'},\n",
       " {'score': 0.08402498066425323,\n",
       "  'token': 10,\n",
       "  'token_str': 'S',\n",
       "  'sequence': 'D L I P T S S K L V V S D T S L Q V K K A F F A L V T'},\n",
       " {'score': 0.07328340411186218,\n",
       "  'token': 8,\n",
       "  'token_str': 'V',\n",
       "  'sequence': 'D L I P T S S K L V V V D T S L Q V K K A F F A L V T'},\n",
       " {'score': 0.06921882927417755,\n",
       "  'token': 12,\n",
       "  'token_str': 'K',\n",
       "  'sequence': 'D L I P T S S K L V V K D T S L Q V K K A F F A L V T'},\n",
       " {'score': 0.06382416933774948,\n",
       "  'token': 11,\n",
       "  'token_str': 'I',\n",
       "  'sequence': 'D L I P T S S K L V V I D T S L Q V K K A F F A L V T'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ProtBERT masked language modeling example: https://huggingface.co/Rostlab/prot_bert\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "unmasker('D L I P T S S K L V V [MASK] D T S L Q V K K A F F A L V T')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
